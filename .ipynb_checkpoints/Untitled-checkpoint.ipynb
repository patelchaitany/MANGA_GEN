{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0101f328-cb5e-418b-b91c-0190f387448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phinex/anaconda3/envs/venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import textgrad as tg\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import langchain\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import SystemMessage, RemoveMessage,AIMessage,HumanMessage,BaseMessage,FunctionMessage\n",
    "from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "import time\n",
    "from prompt import writer_prompt\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import tqdm\n",
    "import pprint\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyCE7RJhTM6Il1Fbf7zr_jsIhfSOLKTga14'\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "#model = ChatGroq(temperature=0, groq_api_key=\"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\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "class responce(BaseModel):\n",
    "    content:List[str] = Field(...,description=\"List Of Generated Story Ideas\")\n",
    "\n",
    "\n",
    "def generate_Example():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",writer_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")]\n",
    "    )\n",
    "    \n",
    "    chain =prompt | model \n",
    "    return chain\n",
    "\n",
    "def get_topic():\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=responce)\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    prompt_m = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Please provide story idea for writing story given user requested topic. \\nGive Responce in JSON with this structure everthing follow the given structure and do give anything other than json.There should be no output before json and after json. \\n\"),\n",
    "        (\"system\",\"{format_instructions}\")\n",
    "        ,MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    prompt_m = prompt_m.partial(format_instructions=format_instructions)\n",
    "\n",
    "    chain = prompt_m |model| parser\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3456e8-4938-42cb-959e-e138c984a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_engine = tg.get_engine(\"gemini-2.0-flash-exp\")\n",
    "tg.set_backward_engine(llm_engine)\n",
    "    \n",
    "STARTING_SYSTEM_PROMPT = writer_prompt\n",
    "system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT, \n",
    "                            requires_grad=True, \n",
    "                            role_description=\"structured system prompt to a somewhat capable language model that will write a story given a topic.\")\n",
    "model = tg.BlackboxLLM(llm_engine, system_prompt)\n",
    "optimizer = tg.TextualGradientDescent(engine=llm_engine, parameters=[system_prompt])\n",
    "evaluation_instruction = (\n",
    "                           \"Evaluate any given answer to this question, \"\n",
    "                           \"be smart, logical, and very critical. \"\n",
    "                           \"Just provide concise feedback.\")\n",
    "\n",
    "loss_system_prompt = \"You are a smart language model that evaluates story from given topic. You do not propose story, only evaluate existing story critically and give very concise feedback.\"\n",
    "loss_system_prompt = tg.Variable(loss_system_prompt, requires_grad=False, role_description=\"system prompt to the loss function\")\n",
    "instruction = \"\"\"\n",
    "    Think About the story topic.\"\"\"\n",
    "\n",
    "format_string = \"{instruction}\\nStory Topic: {{topic}}\\nCurrent story: {{responce_story}}.\\n\"\n",
    "format_string = format_string.format(instruction=instruction)\n",
    "fields = {\"topic\":None, \"responce_story\":None}\n",
    "formatted_llm_call = tg.autograd.FormattedLLMCall(engine=llm_engine,\n",
    "                                                  format_string=format_string,\n",
    "                                                  fields=fields,\n",
    "                                                  system_prompt=loss_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef86a124-c492-4593-aca8-b2f03dcc930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_optimization(data):\n",
    "\n",
    "\n",
    "    def loss_fn(question:tg.Variable, answer:tg.Variable,responce:tg.Variable)->tg.Variable:\n",
    "        input = {\"topic\":question,\"responce_story\":responce}\n",
    "\n",
    "        return formatted_llm_call(inputs = input,response_role_description=f\"evaluation of the {system_prompt.get_role_description()}\")\n",
    "\n",
    "    with open(\"prompt.jsonl\",\"w\") as f:\n",
    "        for epoch in range(3):\n",
    "            loss_f = []\n",
    "            optimizer.zero_grad()\n",
    "            for i in tqdm.tqdm(data):\n",
    "                question_t = i[\"topic\"]\n",
    "                answer_t = i[\"story\"]\n",
    "                question = tg.Variable(question_t,requires_grad=False,role_description=\"topic\")\n",
    "                answer = tg.Variable(answer_t,requires_grad=False,role_description=\"actual story\")\n",
    "                result = model(question)\n",
    "                loss = loss_fn(question = question,answer=answer,responce=result)\n",
    "                loss.generate_graph()\n",
    "                loss_f.append(loss)\n",
    "                break\n",
    "            total_loss = tg.sum(loss_f)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            break\n",
    "            print((system_prompt.value))\n",
    "            f.write(json.dumps({\"epoch\":epoch,\"system_prompt\":system_prompt.value}))\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c2a602-4bfc-4b64-8a6b-1773119db7a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Engine already set. Use override=True to override cautiously.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data_temp:\n\u001b[1;32m      7\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(i))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mprompt_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mprompt_optimization\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_optimization\u001b[39m(data):\n\u001b[1;32m      3\u001b[0m     llm_engine \u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39mget_engine(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash-exp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_backward_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     STARTING_SYSTEM_PROMPT \u001b[38;5;241m=\u001b[39m writer_prompt\n\u001b[1;32m      7\u001b[0m     system_prompt \u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39mVariable(STARTING_SYSTEM_PROMPT, \n\u001b[1;32m      8\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      9\u001b[0m                             role_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured system prompt to a somewhat capable language model that will write a story given a topic.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/venv/lib/python3.12/site-packages/textgrad/config.py:49\u001b[0m, in \u001b[0;36mset_backward_engine\u001b[0;34m(engine, override, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(engine, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     48\u001b[0m     engine \u001b[38;5;241m=\u001b[39m get_engine(engine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m \u001b[43msingleton_backward_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/venv/lib/python3.12/site-packages/textgrad/config.py:33\u001b[0m, in \u001b[0;36mSingletonBackwardEngine.set_engine\u001b[0;34m(self, engine, override)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03mSets the backward engine.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m:return: None\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m override)):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine already set. Use override=True to override cautiously.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n",
      "\u001b[0;31mException\u001b[0m: Engine already set. Use override=True to override cautiously."
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "with open(\"topic.jsonl\") as f:\n",
    "        \n",
    "    data_temp = f.readlines()\n",
    "    for i in data_temp:\n",
    "        data.append(json.loads(i))\n",
    "\n",
    "\n",
    "prompt_optimization(data[:30])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
